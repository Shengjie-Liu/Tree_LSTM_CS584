{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Package\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "import warnings, json, time\n",
    "from IPython import display\n",
    "from datetime import datetime\n",
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt\n",
    "from collections import namedtuple\n",
    "from collections import defaultdict\n",
    "# from textblob import TextBlob\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from pyspark import SparkContext\n",
    "from pyspark.streaming import StreamingContext\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as sf\n",
    "\n",
    "# GLOBAL Var\n",
    "sns.set()\n",
    "warnings.filterwarnings('ignore') \n",
    "nltk.download('wordnet')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# initialize spark\n",
    "sc = SparkContext()\n",
    "spark = SparkSession(sc)\n",
    "sc.setLogLevel(\"ERROR\")\n",
    "sqlContext = SQLContext(sc)\n",
    "ssc = StreamingContext(sc, 1)\n",
    "data_stream = ssc.socketTextStream(\"127.0.0.1\", 9991)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions\n",
    "TWEET_SCHEMA = namedtuple(\"record\", (\"Time_Index\", \"Tweet_Text\"))\n",
    "\n",
    "# split sentence into words\n",
    "def word_TokenizeFunct(x):\n",
    "    return [word for line in x for word in line.split()]\n",
    "\n",
    "# lemmatization\n",
    "def lemmatizationFunct(x):\n",
    "    tmp_ = [lemmatizer.lemmatize(s) for s in x]\n",
    "    return \" \".join(x)\n",
    "\n",
    "# extract noun phrases\n",
    "def extractPhraseFunct(x):\n",
    "\n",
    "    def leaves(tree):\n",
    "        \"\"\"Finds NP (nounphrase) leaf nodes of a chunk tree.\"\"\"\n",
    "        for subtree in tree.subtrees(filter = lambda t: t.label()=='NP'):\n",
    "            yield subtree.leaves()\n",
    "    \n",
    "    def get_terms(tree):\n",
    "        for leaf in leaves(tree):\n",
    "            term = [w for w,t in leaf if not w in stop_words]\n",
    "            yield term\n",
    "    \n",
    "    sentence_re = r'(?:(?:[A-Z])(?:.[A-Z])+.?)|(?:\\w+(?:-\\w+)*)|(?:\\$?\\d+(?:.\\d+)?%?)|(?:...|)(?:[][.,;\"\\'?():-_`])'\n",
    "    grammar = r\"\"\"\n",
    "    NBAR:\n",
    "        {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns\n",
    "        \n",
    "    NP:\n",
    "        {<NBAR>}\n",
    "        {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...\n",
    "    \"\"\"\n",
    "    \n",
    "    chunker = nltk.RegexpParser(grammar)\n",
    "    tokens = nltk.regexp_tokenize(x,sentence_re)\n",
    "    postoks = nltk.tag.pos_tag(tokens) #Part of speech tagging \n",
    "    tree = chunker.parse(postoks) #chunking\n",
    "    terms = get_terms(tree)\n",
    "    temp_phrases = []\n",
    "    for term in terms:\n",
    "        if len(term):\n",
    "            temp_phrases.append(' '.join(term))\n",
    "    \n",
    "    return [w for w in temp_phrases if w] #remove empty lists\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Listening To The Port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spark listen to stream data\n",
    "trip_update = data_stream.map(lambda x: json.loads(x))\\\n",
    "                         .map(lambda x: TWEET_SCHEMA(x[\"Time_Index\"], x[\"Tweet_Text\"]))\\\n",
    "                         .foreachRDD(lambda rdd: rdd.toDF().registerTempTable(\"record\"))\n",
    "\n",
    "# trip_update.pprint()\n",
    "ssc.start()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Monitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP1 LINE PLOT\n",
    "\n",
    "df = pd.DataFrame(columns = [\"TimeStamp\", \"Polarity\"])\n",
    "\n",
    "count = 0\n",
    "res = []\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        tweetRecord = sqlContext.sql(\"SELECT Tweet_Text from record\").rdd.map(lambda x: TextBlob(str(x)).sentences[0].sentiment.polarity)\n",
    "        res += tweetRecord.collect()\n",
    "        df = pd.DataFrame(columns = [\"TimeStamp\", \"Polarity\"])\n",
    "        df.TimeStamp = list(range(0, len(res)))\n",
    "        df.Polarity = res\n",
    "        \n",
    "        # graph\n",
    "        plt.figure(figsize=(16, 6))\n",
    "\n",
    "        display.clear_output(wait = True)\n",
    "#         df.plot.line(x='TimeStamp', y='Polarity')\n",
    "        ax = sns.lineplot(x=\"TimeStamp\", y=\"Polarity\", data=df)\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as inst:\n",
    "        print(type(inst))    # the exception instance\n",
    "        print(inst.args)     # arguments stored in .args\n",
    "        print(inst)  \n",
    "\n",
    "    count += 1\n",
    "    time.sleep(1)\n",
    "    if count == 50:\n",
    "        break\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# APP2 WORD CLOUD\n",
    "\n",
    "wc_schema = StructType([StructField(\"Phrase\", StringType(), True), StructField(\"Count\", FloatType(), True)])\n",
    "wc_df = spark.createDataFrame(sc.emptyRDD(), schema = wc_schema)\n",
    "count = 0\n",
    "\n",
    "while True:\n",
    "    \n",
    "    if count % 10000 == 0:\n",
    "        wc_df = spark.createDataFrame(wc_df.head(30), schema = wc_schema)\n",
    "    else:\n",
    "        print(\"begin\")\n",
    "        try:\n",
    "            # extract tweet content\n",
    "            tweetRecord = sqlContext.sql(\"SELECT Tweet_Text from record\").rdd\\\n",
    "                                    .map(lambda x: word_TokenizeFunct(x))\\\n",
    "                                    .map(lambda x: lemmatizationFunct(x))\\\n",
    "                                    .map(lambda x: extractPhraseFunct(x))\n",
    "\n",
    "            # get phrases/wrods frequency\n",
    "            freqDistRDD = tweetRecord.flatMap(lambda x : nltk.FreqDist(x).most_common())\\\n",
    "                                     .map(lambda x: x)\\\n",
    "                                     .reduceByKey(lambda x,y : x+y)\n",
    "\n",
    "            # aggregate wc\n",
    "            wc_df = wc_df.union(freqDistRDD.toDF())\\\n",
    "                         .groupBy(\"Phrase\")\\\n",
    "                         .agg(sf.sum('Count').alias('Count'))\\\n",
    "                         .sort(sf.col(\"Count\").desc())\n",
    "\n",
    "            # bar plot\n",
    "            wc_df.createOrReplaceTempView(\"AGGTable\") \n",
    "            wc_df_ = spark.sql(\"SELECT Phrase AS Keyword, Count as Frequency from AGGTable limit 20\") #renaming columns \n",
    "            pandD = wc_df_.toPandas()\n",
    "            display.clear_output(wait=True)\n",
    "            \n",
    "            wordcloudConvertDF = pandD.set_index('Keyword').T.to_dict('records')\n",
    "            wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5, colormap='Dark2').generate_from_frequencies(dict(*wordcloudConvertDF))\n",
    "            plt.figure(figsize=(14, 10))    \n",
    "            plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "            \n",
    "        except Exception as inst:\n",
    "            print(type(inst))    # the exception instance\n",
    "            print(inst.args)     # arguments stored in .args\n",
    "            print(inst)  \n",
    "\n",
    "        \n",
    "    count += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
