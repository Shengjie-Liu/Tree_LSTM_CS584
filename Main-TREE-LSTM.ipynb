{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration/Prediction with TreeLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: we suppress training to only one epoch for conciseness of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# in-house package\n",
    "from Utils import *\n",
    "from Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables\n",
    "BATCH_SIZE = 200\n",
    "WORD_DIM = 200\n",
    "BEST_VALID_LOSS = float('inf')\n",
    "EMB_FILE = 'staticFile/glove.6B.' + str(WORD_DIM) + 'd.txt'\n",
    "VOCAB_FILE = 'staticFile/vocab_kaggle.txt'\n",
    "TRAIN_FILE = 'datasets/kaggle_train_tree.txt'\n",
    "TEST_FILE = 'datasets/kaggle_test_tree.txt'\n",
    "DEVICE = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load data\n",
    "trees, embeddings, vocab = text2DGL(TRAIN_FILE, VOCAB_FILE, EMB_FILE, WORD_DIM)\n",
    "itos = dict()\n",
    "for key in vocab.keys(): itos[vocab[key]] = key\n",
    "total_trees = len(trees); ratio = 0.1\n",
    "num_train = int(total_trees * (1. - ratio))\n",
    "# training/dev spliting\n",
    "train_loader = DataLoader(dataset = trees[:num_train], batch_size = BATCH_SIZE, collate_fn = batcher(DEVICE))\n",
    "dev_loader = DataLoader(dataset = trees[num_train:], batch_size = BATCH_SIZE, collate_fn = batcher(DEVICE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Att'n Tree-LSTM (Pre-Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 3; DROPOUT = 0.5\n",
    "# create the model\n",
    "ATT_TYPE = \"ADD\"\n",
    "model = TreeLSTM(len(vocab), WORD_DIM, WORD_DIM, NUM_CLASS, DROPOUT, ATT_TYPE).to(DEVICE)\n",
    "# use pretrained word emb\n",
    "model.embedding.weight.data.copy_(torch.FloatTensor(embeddings).to(DEVICE))\n",
    "model.embedding.weight.requires_grad = True\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 179.7654 | Train Acc 0.5771 |\n",
      "Epoch 00000 | Dev Loss 141.6150 | Dev Acc 0.7124 |\n"
     ]
    }
   ],
   "source": [
    "# 3) start training\n",
    "NUM_EPOCH = 1\n",
    "for i in range(NUM_EPOCH):\n",
    "    loss, acc = training_treeLSTM(model, train_loader, optimizer, DEVICE, False)\n",
    "    loss_v, acc_v = evaluation_treeLSTM(model, dev_loader, DEVICE)\n",
    "    if loss_v < BEST_VALID_LOSS:\n",
    "        BEST_VALID_LOSS = loss_v\n",
    "        torch.save(model.state_dict(), 'text_tree_model_attn.pt')\n",
    "        word_emb = saveWordVecFromModel(itos, model.embedding.weight.data)\n",
    "    \n",
    "    print(\"Epoch {:05d} | Train Loss {:.4f} | Train Acc {:.4f} |\".format(i, loss, acc))\n",
    "    print(\"Epoch {:05d} | Dev Loss {:.4f} | Dev Acc {:.4f} |\".format(i, loss_v, acc_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-LSTM + Att'n + Enhanced (Pre-Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 3; DROPOUT = 0.5\n",
    "# create the model\n",
    "ATT_TYPE = \"ADD\"\n",
    "model = TreeLSTM(len(vocab), WORD_DIM, WORD_DIM, NUM_CLASS, DROPOUT, ATT_TYPE).to(DEVICE)\n",
    "# use pretrained word emb\n",
    "model.embedding.weight.data.copy_(torch.FloatTensor(embeddings).to(DEVICE))\n",
    "model.embedding.weight.requires_grad = True\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 461.7851 | Train Acc 0.5570 |\n",
      "Epoch 00000 | Dev Loss 324.3966 | Dev Acc 0.6603 |\n"
     ]
    }
   ],
   "source": [
    "# 3) start training\n",
    "NUM_EPOCH = 1\n",
    "for i in range(NUM_EPOCH):\n",
    "    loss, acc = training_treeLSTM(model, train_loader, optimizer, DEVICE, True)\n",
    "    loss_v, acc_v = evaluation_treeLSTM(model, dev_loader, DEVICE, True)\n",
    "    if loss_v < BEST_VALID_LOSS:\n",
    "        BEST_VALID_LOSS = loss_v\n",
    "        torch.save(model.state_dict(), 'text_tree_model_attn_plus.pt')\n",
    "        word_emb = saveWordVecFromModel(itos, model.embedding.weight.data)\n",
    "    \n",
    "    print(\"Epoch {:05d} | Train Loss {:.4f} | Train Acc {:.4f} |\".format(i, loss, acc))\n",
    "    print(\"Epoch {:05d} | Dev Loss {:.4f} | Dev Acc {:.4f} |\".format(i, loss_v, acc_v))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree-LSTM + Att'n + Enhanced (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_FILE = 'staticFile/vocab_covid.txt'\n",
    "TRAIN_FILE = 'datasets/covid_training_dgl.txt'\n",
    "TEST_FILE = 'datasets/covid_test_dgl.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load data\n",
    "trees, embeddings, vocab_covid = text2DGL(TRAIN_FILE, VOCAB_FILE, EMB_FILE, WORD_DIM)\n",
    "itos_covid = dict()\n",
    "for key in vocab_covid.keys(): itos_covid[vocab_covid[key]] = key\n",
    "total_trees = len(trees); ratio = 0.1\n",
    "num_train = int(total_trees * (1. - ratio))\n",
    "# training/dev spliting\n",
    "train_loader = DataLoader(dataset = trees[:num_train], batch_size = BATCH_SIZE, collate_fn = batcher(DEVICE))\n",
    "dev_loader = DataLoader(dataset = trees[num_train:], batch_size = BATCH_SIZE, collate_fn = batcher(DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pre-trained word_embeddings\n",
    "updated_wv = updateWordVecToModel(itos_covid, torch.Tensor(embeddings), word_emb, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASS = 3; DROPOUT = 0.5\n",
    "# create the model\n",
    "ATT_TYPE = \"ADD\"\n",
    "model_covid = TreeLSTM(len(vocab_covid), WORD_DIM, WORD_DIM, NUM_CLASS, DROPOUT, ATT_TYPE).to(DEVICE)\n",
    "# use pretrained word emb\n",
    "model_covid.embedding.weight.data.copy_(torch.FloatTensor(updated_wv).to(DEVICE))\n",
    "model_covid.embedding.weight.requires_grad = True\n",
    "# create the optimizer\n",
    "optimizer = torch.optim.Adam(model_covid.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00000 | Train Loss 374.2436 | Train Acc 0.8061 |\n",
      "Epoch 00000 | Dev Loss 309.3046 | Dev Acc 0.8122 |\n"
     ]
    }
   ],
   "source": [
    "# 3) start training\n",
    "NUM_EPOCH = 1\n",
    "BEST_VALID_LOSS = float('inf')\n",
    "for i in range(NUM_EPOCH):\n",
    "    loss, acc = training_treeLSTM(model_covid, train_loader, optimizer, DEVICE, True)\n",
    "    loss_v, acc_v = evaluation_treeLSTM(model_covid, dev_loader, DEVICE, True)\n",
    "    if acc_v < BEST_VALID_LOSS:\n",
    "        BEST_VALID_LOSS = acc_v\n",
    "        torch.save(model.state_dict(), 'covid_tree_attn_plus.pt')\n",
    "    \n",
    "    print(\"Epoch {:05d} | Train Loss {:.4f} | Train Acc {:.4f} |\".format(i, loss, acc))\n",
    "    print(\"Epoch {:05d} | Dev Loss {:.4f} | Dev Acc {:.4f} |\".format(i, loss_v, acc_v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
