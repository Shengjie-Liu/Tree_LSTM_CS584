{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calibration/Prediction with LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice: we suppress training to only one epoch for conciseness of the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# built-in pacakges\n",
    "import torch.optim as optim\n",
    "# in-house pacakages\n",
    "from Utils import *\n",
    "from Models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VAR\n",
    "DEVICE = torch.device(\"cuda\")\n",
    "TRAIN_FILE_NAME = \"datasets/kaggle_train\"\n",
    "TEST_FILE_NAME = \"datasets/kaggle_test\"\n",
    "BATCH_SIZE = 200\n",
    "EMB_SIZE = 100\n",
    "BEST_VALID_LOSS = float('inf')\n",
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data\n",
    "train_iter, valid_iter, test_iter, text = tabularData(TRAIN_FILE_NAME, TEST_FILE_NAME, DEVICE, BATCH_SIZE, EMB_SIZE)\n",
    "vocab = text.vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Att'n Bi-Direction LSTM (Pre-Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss(pre): 0.944 | Train Acc: 53.51%\n",
      "\t Val. Loss: 0.751 |  Val. Acc: 67.88%\n"
     ]
    }
   ],
   "source": [
    "# set up model/loss fun\n",
    "VOCAB_SIZE = len(vocab)\n",
    "WORD_DIM = EMB_SIZE; HID_DIM = 200; BIDIR = True\n",
    "NUM_LAYERS = 2; NUM_CLASS = 3; ATT_DIM = 30; DROPOUT = 0.5\n",
    "PAD_IDX = vocab.stoi[text.pad_token]; UNK_IDX = vocab.stoi[text.unk_token]\n",
    "# initialize model\n",
    "model = RNN(VOCAB_SIZE, WORD_DIM, HID_DIM, NUM_CLASS, NUM_LAYERS, BIDIR, DROPOUT, PAD_IDX, DEVICE, ATT_DIM).to(DEVICE)\n",
    "# initialize word embedding\n",
    "model.embedding.weight.data.copy_(vocab.vectors)\n",
    "model.embedding.weight.data[UNK_IDX] = torch.zeros(WORD_DIM)\n",
    "model.embedding.weight.data[PAD_IDX] = torch.zeros(WORD_DIM)\n",
    "# initialize optimizer/loss functionals\n",
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# train model\n",
    "EPOCHS = 1\n",
    "BEST_VALID_LOSS = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_att = training_LSTM(model, train_iter, optimizer, criterion, DEVICE)\n",
    "    valid_loss, valid_acc = evaluation_LSTM(model, valid_iter, criterion)\n",
    "    if valid_loss < BEST_VALID_LOSS:\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "#         torch.save(model.state_dict(), 'trained_model/text_lstm_model_attn.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02}')\n",
    "    print(f'\\tTrain Loss(pre): {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Direction LSTM + Att'n + Enhanced (Pre-Training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss(pre): 0.953 | Train Loss(att): 1.665 | Train Acc: 52.09%\n",
      "\t Val. Loss: 0.823 |  Val. Acc: 60.16%\n"
     ]
    }
   ],
   "source": [
    "# initialize model\n",
    "model_plus = RNN(VOCAB_SIZE, WORD_DIM, HID_DIM, NUM_CLASS, NUM_LAYERS, BIDIR, DROPOUT, PAD_IDX, DEVICE, ATT_DIM, True).to(DEVICE)\n",
    "# initialize word embedding\n",
    "model_plus.embedding.weight.data.copy_(vocab.vectors)\n",
    "model_plus.embedding.weight.data[UNK_IDX] = torch.zeros(WORD_DIM)\n",
    "model_plus.embedding.weight.data[PAD_IDX] = torch.zeros(WORD_DIM)\n",
    "# initialize optimizer/loss functionals\n",
    "optimizer = optim.Adam(model_plus.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)\n",
    "\n",
    "# train model\n",
    "EPOCHS = 1\n",
    "BEST_VALID_LOSS = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_att = training_LSTM(model_plus, train_iter, optimizer, criterion, DEVICE)\n",
    "    valid_loss, valid_acc = evaluation_LSTM(model_plus, valid_iter, criterion)\n",
    "    if valid_loss < BEST_VALID_LOSS:\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "        word_emb = saveWordVecFromModel(vocab.itos, model_plus.embedding.weight.data)\n",
    "#         torch.save(model_plus.state_dict(), 'trained_model/text_lstm_model_attn_plus.pt')\n",
    " \n",
    "    print(f'Epoch: {epoch + 1:02}')\n",
    "    print(f'\\tTrain Loss(pre): {train_loss:.3f} | Train Loss(att): {train_att:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bi-Direction LSTM + Att'n + Enhanced (Fine-Tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch Dataset\n",
    "TRAIN_FILE_NAME = \"datasets/covid_train\"\n",
    "TEST_FILE_NAME = \"datasets/covid_test\"\n",
    "# prepare data\n",
    "train_iter, valid_iter, test_iter, text_covid = tabularData(TRAIN_FILE_NAME, TEST_FILE_NAME, DEVICE, BATCH_SIZE, EMB_SIZE, 0.9, 1, False)\n",
    "vocab_covid = text_covid.vocab\n",
    "# load pre-trained word_embeddings\n",
    "updated_wv = updateWordVecToModel(vocab_covid.itos, vocab_covid.vectors, word_emb, DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# construct model\n",
    "VOCAB_SIZE = len(vocab_covid)\n",
    "WORD_DIM = EMB_SIZE; HID_DIM = 200; BIDIR = True\n",
    "NUM_LAYERS = 2; NUM_CLASS = 3; ATT_DIM = 30; DROPOUT = 0.5\n",
    "PAD_IDX = vocab_covid.stoi[text_covid.pad_token]; UNK_IDX = vocab_covid.stoi[text_covid.unk_token]\n",
    "# initialize model\n",
    "model_covid = RNN(VOCAB_SIZE, WORD_DIM, HID_DIM, NUM_CLASS, NUM_LAYERS, BIDIR, DROPOUT, PAD_IDX, DEVICE, ATT_DIM).to(DEVICE)\n",
    "model_covid.embedding.weight.data.copy_(updated_wv)\n",
    "model_covid.embedding.weight.data[UNK_IDX] = torch.zeros(WORD_DIM)\n",
    "model_covid.embedding.weight.data[PAD_IDX] = torch.zeros(WORD_DIM)\n",
    "# initialize optimizer/loss functionals\n",
    "optimizer = optim.Adam(model_covid.parameters())\n",
    "criterion = nn.CrossEntropyLoss().to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01\n",
      "\tTrain Loss(pre): 0.917 | Train Acc: 56.56%\n",
      "\t Val. Loss: 0.696 |  Val. Acc: 73.28%\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "EPOCHS = 1\n",
    "BEST_VALID_LOSS = float('inf')\n",
    "for epoch in range(EPOCHS):\n",
    "    train_loss, train_acc, train_att = training_LSTM(model_covid, train_iter, optimizer, criterion, DEVICE)\n",
    "    valid_loss, valid_acc = evaluation_LSTM(model_covid, valid_iter, criterion)\n",
    "    if valid_loss < BEST_VALID_LOSS:\n",
    "        BEST_VALID_LOSS = valid_loss\n",
    "#         torch.save(model_covid.state_dict(), 'trained_model/lstm_covid19.pt')\n",
    "\n",
    "    print(f'Epoch: {epoch + 1:02}')\n",
    "    print(f'\\tTrain Loss(pre): {train_loss:.3f} | Train Acc: {train_acc * 100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1 = RNN(VOCAB_SIZE, WORD_DIM, HID_DIM, NUM_CLASS, NUM_LAYERS, BIDIR, DROPOUT, PAD_IDX, DEVICE, ATT_DIM).to(DEVICE)\n",
    "model_1.load_state_dict(torch.load(\"trained_model/lstm_covid19.pt\"), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style  type=\"text/css\" >\n",
       "    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col0 {\n",
       "            background-color:  #ffe5e5;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col1 {\n",
       "            background-color:  #ffe2e2;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col2 {\n",
       "            background-color:  #ffdfdf;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col3 {\n",
       "            background-color:  #ffcece;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col4 {\n",
       "            background-color:  #ffd7d7;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col5 {\n",
       "            background-color:  #ffe4e4;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col6 {\n",
       "            background-color:  #ffe7e7;\n",
       "        }    #T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col7 {\n",
       "            background-color:  #ffe7e7;\n",
       "        }</style><table id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382\" ><thead>    <tr>        <th class=\"blank level0\" ></th>        <th class=\"col_heading level0 col0\" >0</th>        <th class=\"col_heading level0 col1\" >1</th>        <th class=\"col_heading level0 col2\" >2</th>        <th class=\"col_heading level0 col3\" >3</th>        <th class=\"col_heading level0 col4\" >4</th>        <th class=\"col_heading level0 col5\" >5</th>        <th class=\"col_heading level0 col6\" >6</th>        <th class=\"col_heading level0 col7\" >7</th>    </tr></thead><tbody>\n",
       "                <tr>\n",
       "                        <th id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col0\" class=\"data row0 col0\" >i</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col1\" class=\"data row0 col1\" >pray</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col2\" class=\"data row0 col2\" >my</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col3\" class=\"data row0 col3\" >haters</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col4\" class=\"data row0 col4\" >die</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col5\" class=\"data row0 col5\" >of</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col6\" class=\"data row0 col6\" >corona</td>\n",
       "                        <td id=\"T_644b2f5e_9308_11ea_8899_94e6f721d382row0_col7\" class=\"data row0 col7\" >virus</td>\n",
       "            </tr>\n",
       "    </tbody></table>"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x211d6a6ae88>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check example\n",
    "inx = 151\n",
    "sen = vars(test_iter.dataset.examples[inx])['text']\n",
    "res, attention = extract_att_weights(model_1, sen, vocab_covid, DEVICE)\n",
    "show_attention(sen, attention)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
